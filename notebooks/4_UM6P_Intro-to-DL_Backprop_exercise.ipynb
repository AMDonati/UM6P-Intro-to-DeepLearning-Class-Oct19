{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Multilayer Neural Networks\n",
    "\n",
    "### In this notebook, you will learn to:\n",
    "- Diving deep: implement a real gradient descent in `Numpy`\n",
    "\n",
    "### Dataset:\n",
    "- Digits: 10 class handwritten digits\n",
    "- [sklearn.datasets.load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAADSCAYAAAAYNx+qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5dc849de10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TO DO: \n",
    "# transform the digits.data and the digits.target into numppy arrays\n",
    "\n",
    "# split the dataset with a test_size=0.15, random_state=37\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "# use the preprocessing.StandardScaler() function of sci-kit learn\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "#Look at the shapes and dtypes of X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation\n",
    "\n",
    "## a) Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD using numpy. Here are the objectives:\n",
    "\n",
    "- Implement a simple forward model with no hidden layer (equivalent to a logistic regression):\n",
    "note: shape, transpose of W with regards to course\n",
    "$y = softmax(\\mathbf{W} \\dot x + b)$\n",
    "\n",
    "- Build a predict function which returns the most probable class given an input $x$\n",
    "\n",
    "- Build an accuracy function for a batch of inputs $X$ and the corresponding expected outputs $y_{true}$\n",
    "\n",
    "- Build a grad function which computes $\\frac{d}{dW} -\\log(softmax(W \\dot x + b))$ for an $x$ and its corresponding expected output $y_{true}$ ; check that the gradients are well defined\n",
    "\n",
    "- Build a train function which uses the grad function output to update $\\mathbf{W}$ and $b$\n",
    "\n",
    "\n",
    "### One-hot encoding for class label data\n",
    "\n",
    "First let's define a function to compute the one hot encoding of an integer array for a fixed number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=[0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "Now let's implement the softmax vector function:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO:\n",
    "    # code the softmax function here using np.exp function. \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to implement softmax that works both for an individual vector of activations and for a batch of activation vectors at once:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities should sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(softmax([10, 2, -3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"sotfmax of 2 vectors:\")\n",
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of probabilities for each input vector of logits should some to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and some predicted probabilities `Y_pred` returns the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.asarray(Y_true)\n",
    "    Y_pred = np.asarray(Y_pred)\n",
    "    # TODO\n",
    "    # implement the negative log likelihood here. \n",
    "    return 0.\n",
    "\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the nll of a very confident yet bad prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be past as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.atleast_2d(Y_true)\n",
    "    Y_pred = np.atleast_2d(Y_pred)\n",
    "\n",
    "    # TODO: compute the negative log_likelihood\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now study the following linear model trainable by SGD, **one sample at a time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #TO DO: implement a forward pass:\n",
    "        #Z=X*self.W+self.b (use np.dot for matrix multiplication)\n",
    "        return softmax(Z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "        # TO DO\n",
    "        # compute the gradient of dnll_output woth respect to W and b.\n",
    "        # grad_w=\n",
    "        # grad_b=\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD\n",
    "        # TO DO\n",
    "        # compute the grads (grad_W, grad_b)\n",
    "        # update self.W: self.W <- self.W - learning_rate * grads[\"W\"]\n",
    "        # update self.W: self.b <- self.b - learning_rate * grads[\"b] \n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(x))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a model and test its forward inference\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "lr = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the randomly initialized model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()\n",
    "    \n",
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Multilayer\n",
    "\n",
    "The objective of this section is to implement the backpropagation algorithm (SGD with the chain rule) on a single layer neural network using the sigmoid activation function.\n",
    "\n",
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO\n",
    "    # implement the sigmoid function using np.exp\n",
    "    return X\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    # TODO\n",
    "    # implement the derivation of the sigmoid function. \n",
    "    return X\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "  - $\\mathbf{h} = sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h})$\n",
    "  - $\\mathbf{y} = softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o})$\n",
    "\n",
    "- Notes: \n",
    "  - try to keep the code as similar as possible as the previous one;\n",
    "  - `forward_keep_activations` is similar to forward, but also returns hidden activations and pre activations;\n",
    "\n",
    "- Update the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- Implement the `train` and `loss` functions.\n",
    "\n",
    "**Bonus**: reimplementing all from scratch only using the lecture slides but without looking at the solution of the `LogisticRegression` is an excellent exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO\n",
    "        # implement the forward pass for a MLP with one hidden layer. \n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            return np.random.uniform(size=self.output_size,\n",
    "                                     high=1.0-EPSILON, low=EPSILON)\n",
    "        else:\n",
    "            return np.random.uniform(size=(X.shape[0], self.output_size),\n",
    "                                     high=1.0-EPSILON, low=EPSILON)\n",
    "    \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO\n",
    "        # replace z_h, h and y by their equations. \n",
    "        z_h = 0.\n",
    "        h = 0.\n",
    "        y = np.random.uniform(size=self.output_size,\n",
    "                              high=1.0-EPSILON, low=EPSILON)\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return 0.\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        return {\"W_h\": 0., \"b_h\": 0., \"W_o\": 0., \"b_o\": 0.}\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(x))\n",
    "        else:\n",
    "            return np.argmax(self.forward(x), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if you want to dig further in the implementation of the backprop algo. \n",
    "- Watch the following video on [how to code a minimal deep learning framework](https://www.youtube.com/watch?v=o64FV-ez6Gw) that feels like a simplified version\n",
    "of Keras but using numpy instead of tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"o64FV-ez6Gw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the following blog post on Reverse-Mode Automatic Differentiation from start to section \"A simple implementation in Python\" included:\n",
    "\n",
    "  https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
