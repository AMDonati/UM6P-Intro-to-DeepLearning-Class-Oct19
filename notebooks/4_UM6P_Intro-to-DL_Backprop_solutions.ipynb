{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Multilayer Neural Networks\n",
    "\n",
    "### In this notebook, you will learn to:\n",
    "- Diving deep: implement a real gradient descent in `Numpy`\n",
    "\n",
    "### Dataset:\n",
    "- Digits: 10 class handwritten digits\n",
    "- [sklearn.datasets.load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMIAAADSCAYAAAAYNx+qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc2449dab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_index = 45\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
    "           interpolation='nearest')\n",
    "plt.title(\"image label: %d\" % digits.target[sample_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "- Normalization\n",
    "- Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TO DO: \n",
    "# transform the digits.data and the digits.target into numppy arrays\n",
    "data = np.asarray(digits.data, dtype='float32')\n",
    "target = np.asarray(digits.target, dtype='int32')\n",
    "\n",
    "# split the dataset with a test_size=0.15, random_state=37\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, target, test_size=0.15, random_state=37)\n",
    "\n",
    "# mean = 0 ; standard deviation = 1.0\n",
    "# use the preprocessing.StandardScaler() function of sci-kit learn\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# print(scaler.mean_)\n",
    "# print(scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at the shapes and dtypes of X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1527, 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy Implementation\n",
    "\n",
    "## a) Logistic Regression\n",
    "\n",
    "In this section we will implement a logistic regression model trainable with SGD using numpy. Here are the objectives:\n",
    "\n",
    "- Implement a simple forward model with no hidden layer (equivalent to a logistic regression):\n",
    "note: shape, transpose of W with regards to course\n",
    "$y = softmax(\\mathbf{W} \\dot x + b)$\n",
    "\n",
    "- Build a predict function which returns the most probable class given an input $x$\n",
    "\n",
    "- Build an accuracy function for a batch of inputs $X$ and the corresponding expected outputs $y_{true}$\n",
    "\n",
    "- Build a grad function which computes $\\frac{d}{dW} -\\log(softmax(W \\dot x + b))$ for an $x$ and its corresponding expected output $y_{true}$ ; check that the gradients are well defined\n",
    "\n",
    "- Build a train function which uses the grad function output to update $\\mathbf{W}$ and $b$\n",
    "\n",
    "\n",
    "### One-hot encoding for class label data\n",
    "\n",
    "First let's define a function to compute the one hot encoding of an integer array for a fixed number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(n_classes, y):\n",
    "    return np.eye(n_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(n_classes=10, y=[0, 4, 9, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "Now let's implement the softmax vector function:\n",
    "\n",
    "$$\n",
    "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "  e^{x_1}\\\\\\\\\n",
    "  e^{x_2}\\\\\\\\\n",
    "  \\vdots\\\\\\\\\n",
    "  e^{x_n}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    # TODO:\n",
    "    # code the softmax function here using np.exp function. \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that this works one vector at a time (and check that the components sum to one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a way to implement softmax that works both for an individual vector of activations and for a batch of activation vectors at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax of a single vector:\n",
      "[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n"
     ]
    }
   ],
   "source": [
    "def softmax(X):\n",
    "    exp = np.exp(X)\n",
    "    return exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "print(\"softmax of a single vector:\")\n",
    "print(softmax([10, 2, -3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities should sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(softmax([10, 2, -3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sotfmax of 2 vectors:\n",
      "[[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n",
      " [2.47262316e-03 9.97527377e-01 1.38536042e-11]]\n"
     ]
    }
   ],
   "source": [
    "print(\"sotfmax of 2 vectors:\")\n",
    "X = np.array([[10, 2, -3],\n",
    "              [-1, 5, -20]])\n",
    "print(softmax(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of probabilities for each input vector of logits should some to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(softmax(X), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that given the true one-hot encoded class `Y_true` and some predicted probabilities `Y_pred` returns the negative log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01005032575249135\n"
     ]
    }
   ],
   "source": [
    "EPSILON=1e-8 # to avoid having log 0 \n",
    "\n",
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.asarray(Y_true)\n",
    "    Y_pred = np.asarray(Y_pred)\n",
    "    # TODO\n",
    "    # implement the negative log likelihood here. \n",
    "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
    "    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
    "    return -np.mean(loglikelihoods)\n",
    "\n",
    "# Make sure that it works for a simple sample at a time\n",
    "print(nll([1, 0, 0], [.99, 0.01, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the nll of a very confident yet bad prediction is a much higher positive number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.605169185988592\n"
     ]
    }
   ],
   "source": [
    "print(nll([1, 0, 0], [0.01, 0.01, .98]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your implementation can compute the average negative log likelihood of a group of predictions: `Y_pred` and `Y_true` can therefore be past as 2D arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nll(Y_true, Y_pred):\n",
    "    Y_true = np.atleast_2d(Y_true)\n",
    "    Y_pred = np.atleast_2d(Y_pred)\n",
    "\n",
    "    # TODO\n",
    "    return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Check that the average NLL of the following 3 almost perfect\n",
    "# predictions is close to 0\n",
    "Y_true = np.array([[0, 1, 0],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1]])\n",
    "\n",
    "Y_pred = np.array([[0,   1,    0],\n",
    "                   [.99, 0.01, 0],\n",
    "                   [0,   0,    1]])\n",
    "\n",
    "print(nll(Y_true, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now study the following linear model trainable by SGD, **one sample at a time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.uniform(size=(input_size, output_size),\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.b = np.random.uniform(size=output_size,\n",
    "                                   high=0.1, low=-0.1)\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #TO DO: implement a forward pass:\n",
    "        #Z=X*self.W+self.b (use np.dot for matrix multiplication)\n",
    "        Z = np.dot(X, self.W) + self.b\n",
    "        return softmax(Z)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "    \n",
    "    def grad_loss(self, x, y_true):\n",
    "        y_pred = self.forward(x)\n",
    "        dnll_output =  y_pred - one_hot(self.output_size, y_true)\n",
    "        # TO DO\n",
    "        # compute the gradient of dnll_output woth respect to W and b.\n",
    "        # grad_w=\n",
    "        # grad_b=\n",
    "        grad_W = np.outer(x, dnll_output)\n",
    "        grad_b = dnll_output\n",
    "        grads = {\"W\": grad_W, \"b\": grad_b}\n",
    "        return grads\n",
    "    \n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD\n",
    "        # TO DO\n",
    "        # compute the grads (grad_W, grad_b)\n",
    "        grads = self.grad_loss(x, y)\n",
    "        # update self.W: self.W <- self.W - learning_rate * grads[\"W\"]\n",
    "        # update self.W: self.b <- self.b - learning_rate * grads[\"b]\n",
    "        self.W = self.W - learning_rate * grads[\"W\"]\n",
    "        self.b = self.b - learning_rate * grads[\"b\"]      \n",
    "        \n",
    "    def loss(self, x, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(x))\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of the untrained model:\n",
      "train loss: 0.0000, train acc: 0.116, test acc: 0.111\n"
     ]
    }
   ],
   "source": [
    "# Build a model and test its forward inference\n",
    "n_features = X_train.shape[1]\n",
    "n_classes = len(np.unique(y_train))\n",
    "lr = LogisticRegression(n_features, n_classes)\n",
    "\n",
    "print(\"Evaluation of the untrained model:\")\n",
    "train_loss = lr.loss(X_train, y_train)\n",
    "train_acc = lr.accuracy(X_train, y_train)\n",
    "test_acc = lr.accuracy(X_test, y_test)\n",
    "\n",
    "print(\"train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the randomly initialized model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAEWCAYAAACADFYuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJztmZmZWaP8ftLVV/YuV3rwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc244b63668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_prediction(model, sample_idx=0, classes=range(10)):\n",
    "    fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
    "\n",
    "    ax0.imshow(scaler.inverse_transform(X_test[sample_idx]).reshape(8, 8), cmap=plt.cm.gray_r,\n",
    "               interpolation='nearest')\n",
    "    ax0.set_title(\"True image label: %d\" % y_test[sample_idx]);\n",
    "\n",
    "\n",
    "    ax1.bar(classes, one_hot(len(classes), y_test[sample_idx]), label='true')\n",
    "    ax1.bar(classes, model.forward(X_test[sample_idx]), label='prediction', color=\"red\")\n",
    "    ax1.set_xticks(classes)\n",
    "    prediction = model.predict(X_test[sample_idx])\n",
    "    ax1.set_title('Output probabilities (prediction: %d)'\n",
    "                  % prediction)\n",
    "    ax1.set_xlabel('Digit class')\n",
    "    ax1.legend()\n",
    "    \n",
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update #0, train loss: 0.0000, train acc: 0.138, test acc: 0.130\n",
      "Update #100, train loss: 0.0000, train acc: 0.712, test acc: 0.730\n",
      "Update #200, train loss: 0.0000, train acc: 0.853, test acc: 0.874\n",
      "Update #300, train loss: 0.0000, train acc: 0.902, test acc: 0.922\n",
      "Update #400, train loss: 0.0000, train acc: 0.908, test acc: 0.926\n",
      "Update #500, train loss: 0.0000, train acc: 0.914, test acc: 0.941\n",
      "Update #600, train loss: 0.0000, train acc: 0.930, test acc: 0.941\n",
      "Update #700, train loss: 0.0000, train acc: 0.936, test acc: 0.948\n",
      "Update #800, train loss: 0.0000, train acc: 0.937, test acc: 0.952\n",
      "Update #900, train loss: 0.0000, train acc: 0.940, test acc: 0.948\n",
      "Update #1000, train loss: 0.0000, train acc: 0.946, test acc: 0.948\n",
      "Update #1100, train loss: 0.0000, train acc: 0.945, test acc: 0.959\n",
      "Update #1200, train loss: 0.0000, train acc: 0.949, test acc: 0.967\n",
      "Update #1300, train loss: 0.0000, train acc: 0.950, test acc: 0.959\n",
      "Update #1400, train loss: 0.0000, train acc: 0.953, test acc: 0.952\n",
      "Update #1500, train loss: 0.0000, train acc: 0.950, test acc: 0.959\n"
     ]
    }
   ],
   "source": [
    "# Training for one epoch\n",
    "learning_rate = 0.01\n",
    "\n",
    "for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "    lr.train(x, y, learning_rate)\n",
    "    if i % 100 == 0:\n",
    "        train_loss = lr.loss(X_train, y_train)\n",
    "        train_acc = lr.accuracy(X_train, y_train)\n",
    "        test_acc = lr.accuracy(X_test, y_test)\n",
    "        print(\"Update #%d, train loss: %0.4f, train acc: %0.3f, test acc: %0.3f\"\n",
    "              % (i, train_loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAEWCAYAAACADFYuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc244af22e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction(lr, sample_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Feedforward Multilayer\n",
    "\n",
    "The objective of this section is to implement the backpropagation algorithm (SGD with the chain rule) on a single layer neural network using the sigmoid activation function.\n",
    "\n",
    "- Implement the `sigmoid` and its element-wise derivative `dsigmoid` functions:\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dsigmoid(x) = sigmoid(x) \\cdot (1 - sigmoid(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-35-8e7798ac70eb>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-8e7798ac70eb>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    return X\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(X):\n",
    "    # TODO\n",
    "    # implement the sigmoid function using np.exp\n",
    "       return 1 / (1 + np.exp(-X))\n",
    "    return X\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    # TODO\n",
    "    # implement the derivation of the sigmoid function. \n",
    "    sig=sigmoid(X)\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, sigmoid(x), label='sigmoid')\n",
    "plt.plot(x, dsigmoid(x), label='dsigmoid')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implement `forward` and `forward_keep_all` functions for a model with a hidden layer with a sigmoid activation function:\n",
    "  - $\\mathbf{h} = sigmoid(\\mathbf{W}^h \\mathbf{x} + \\mathbf{b^h})$\n",
    "  - $\\mathbf{y} = softmax(\\mathbf{W}^o \\mathbf{h} + \\mathbf{b^o})$\n",
    "\n",
    "- Notes: \n",
    "  - try to keep the code as similar as possible as the previous one;\n",
    "  - `forward_keep_activations` is similar to forward, but also returns hidden activations and pre activations;\n",
    "\n",
    "- Update the grad function to compute all gradients; check that the gradients are well defined;\n",
    "\n",
    "- Implement the `train` and `loss` functions.\n",
    "\n",
    "**Bonus**: reimplementing all from scratch only using the lecture slides but without looking at the solution of the `LogisticRegression` is an excellent exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPSILON = 1e-8\n",
    "\n",
    "\n",
    "class NeuralNet():\n",
    "    \"\"\"MLP with 1 hidden layer with a sigmoid activation\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # TODO\n",
    "        self.W_h = None\n",
    "        self.b_h = None\n",
    "        self.W_o = None\n",
    "        self.b_o = None\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # TODO\n",
    "        # implement the forward pass for a MLP with one hidden layer. \n",
    "        h = sigmoid(np.dot(X, self.W_h) + self.b_h)\n",
    "        y = softmax(np.dot(h, self.W_o) + self.b_o)\n",
    "        \n",
    "        if len(X.shape) == 1:\n",
    "            return np.random.uniform(size=self.output_size,\n",
    "                                     high=1.0-EPSILON, low=EPSILON)\n",
    "        else:\n",
    "            return np.random.uniform(size=(X.shape[0], self.output_size),\n",
    "                                     high=1.0-EPSILON, low=EPSILON)\n",
    "    \n",
    "    def forward_keep_activations(self, X):\n",
    "        # TODO\n",
    "        # replace z_h, h and y by their equations. \n",
    "        z_h = np.dot(X, self.W_h) + self.b_h\n",
    "        h = sigmoid(z_h)\n",
    "        z_o = np.dot(h, self.W_o) + self.b_o\n",
    "        y = softmax(z_o)\n",
    "        return y, h, z_h\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        # TODO\n",
    "        return 0.\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        # TODO\n",
    "        y, h, z_h = self.forward_keep_activations(X)\n",
    "        grad_z_o = y - one_hot(self.output_size, y_true)\n",
    "\n",
    "        grad_W_o = np.outer(h, grad_z_o)\n",
    "        grad_b_o = grad_z_o\n",
    "        grad_h = np.dot(grad_z_o, np.transpose(self.W_o))\n",
    "        grad_z_h = grad_h * dsigmoid(z_h)\n",
    "        grad_W_h = np.outer(x, grad_z_h)\n",
    "        grad_b_h = grad_z_h\n",
    "        grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "                 \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return {\"W_h\": 0., \"b_h\": 0., \"W_o\": 0., \"b_o\": 0.}\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # TODO\n",
    "        # Traditional SGD update on one sample at a time\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W_h = self.W_h - learning_rate * grads[\"W_h\"]\n",
    "        self.b_h = self.b_h - learning_rate * grads[\"b_h\"]\n",
    "        self.W_o = self.W_o - learning_rate * grads[\"W_o\"]\n",
    "        self.b_o = self.b_o - learning_rate * grads[\"b_o\"]\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(x))\n",
    "        else:\n",
    "            return np.argmax(self.forward(x), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "model = NeuralNet(n_features, n_hidden, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.loss(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.accuracy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses, accuracies, accuracies_test = [], [], []\n",
    "losses.append(model.loss(X_train, y_train))\n",
    "accuracies.append(model.accuracy(X_train, y_train))\n",
    "accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "\n",
    "print(\"Random init: train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "      % (losses[-1], accuracies[-1], accuracies_test[-1]))\n",
    "\n",
    "for epoch in range(15):\n",
    "    for i, (x, y) in enumerate(zip(X_train, y_train)):\n",
    "        model.train(x, y, 0.1)\n",
    "\n",
    "    losses.append(model.loss(X_train, y_train))\n",
    "    accuracies.append(model.accuracy(X_train, y_train))\n",
    "    accuracies_test.append(model.accuracy(X_test, y_test))\n",
    "    print(\"Epoch #%d, train loss: %0.5f, train acc: %0.3f, test acc: %0.3f\"\n",
    "          % (epoch + 1, losses[-1], accuracies[-1], accuracies_test[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(\"Training loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(accuracies, label='train')\n",
    "plt.plot(accuracies_test, label='test')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_prediction(model, sample_idx=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## if you want to dig further in the implementation of the backprop algo. \n",
    "- Watch the following video on [how to code a minimal deep learning framework](https://www.youtube.com/watch?v=o64FV-ez6Gw) that feels like a simplified version\n",
    "of Keras but using numpy instead of tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"o64FV-ez6Gw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the following blog post on Reverse-Mode Automatic Differentiation from start to section \"A simple implementation in Python\" included:\n",
    "\n",
    "  https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
